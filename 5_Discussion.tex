\chapter{Discussion}
\label{discussion}

We devote this chapter to a deeper look at some of factors that may (or may not)
shine a light on a few of the more perplexing and intriguing observations made
through the user studies mentioned in the previous chapter, as well as a
meta-review of what, if anything, we may conclude based on our reported data and
observations. We spend the most attention on our latest study (with the PopCAD
and SnapCAD) as it is not only the most recent, but the most significant, as it
concerns not only whether or not our devices can be used effectively for
modeling by novice users, but touches on the relationships between our devices,
spatial reasoning, and embodied cognition. We start with the significance of the
gesture and speech observations made in the PopCAD and SnapCAD study, discuss
the role of age in relation to modeling performance, followed by an analysis of
the effects of shape complexity on modeling acuity, and finally some
meta-analysis of the observed data over the three user studies described in the
last chapter.


\section{Gesture and Speech Significance}

The work of Ehrlich\cite{ehrlich2006importance}, Levine\cite{levine1999early},
and Goldin-Meadow especially\cite{goldin}\cite{goldin2005hearing}, serves as a
rough guide to our most recent study design, as they touch on the role of
gesture in determining spatial reasoning performance, and later provide strong
evidence that gesture is a valuable window into the mind, all of which supports
the notions inherent in embodied cognition - that body and mind are far more
tightly linked than we have traditionally be led to believe. As we operate under
these assumptions as reasoning to create tangible, physically involved
interfaces (as opposed to pure 2D software) it is worth taking a deeper look
into how our study results compare and contrast with this earlier work.

Ehrlich and Levine's studies focus on the gestures and speech produced during
children's explanations of how they solved a series of mental transformation
tasks. The participants were presented with the same sorts of instruments used
in our PopCAD and SnapCAD study, although our studies differ significantly in
several ways. Of course, Ehrlich and Levine had no devices, and evaluated the
speech and gestures produced in explaining the mental transformation task,
whereas we examined the strategies expressed when modeling on the PopCAD and
SnapCAD. Apart from one practice condition where wooden blocks were used (which
in their study had no effect) all of the tasks in \cite{ehrlich2006importance}
were based on 2D paper representations, and the subject had no physical contact
with any of the objects they were trying to model. In our study, subjects were
handed a 3D-printed (and thus 3D) model of the shape they were attempting to
reproduce. Additionally, in Ehrlich and Levine's studies, subjects were
instructed to (in their mind) ``move the pieces together'' or to ``observe the
movement'' of the pieces as manipulated by the experimenter. These factors, as
well as differences in age (our subject population was 5-13 years older than
those in \cite{levine1999early} and \cite{ehrlich2006importance}) likely
contribute to (at least of some of) the differences observed in our study. We
spend the next two subsections dissecting the similarities and contrasts in our
results compared to the finding reported by Ehrlich, Goldin-Meadow, and Levine.


\subsection{Contrasts}

Given the differences in the nature of our study compared with Ehrlich, that our
finding should differ should come as no surprise. While both of the gesture and
speech analyses occurred while subjects were explaining a modeling strategy, the
type of modeling activity they had been asked to perform was substantively
dissimilar. As noted above, subjects in our study were handed physical, 3D
models of the target shape they were tasked with modeling, and could hold on to
that object (and rotate it, look at it from different angles, hold it in front
of the device or the computer screen, etc.) during their modeling process.
Afterward, when asked about their strategy, they still had that object, and
often gestured to it (or with it) and (of course) talked about it. No such
``hands-on'' activity was involved in the studies we reference above, nor (as
we note later in the chapter) could we find substantive work involving such
manipulative activities to examine spatial reasoning ability. 

We contend that the ``embodied'' nature of the tasks in our study help explain
some of the differences in gesture and speech patterns and correlations that we
observed. In fact, it is likely that given Goldin-Meadow's body of work and
studies involving cognition and gesture, she would concur with us. Furthermore,
the way in which the examiner in the above studies introduces the tasks to the
subjects involves several direct references to movement (e.g. ``In your mind,
move the pieces together and then move them back apart.''). This is significant,
as gestures and speech relating to movement was (in their study) both the most
frequent type of strategy expressed, but, as far as gesture correlations with
task performance is concerned, gestures coded as relating to movement was the
only type of response they recorded that was exclusively related to answering
the test questions correctly. To take an excerpt from
\cite{ehrlich2006importance} (pp. 1265):
\begin{quote}
``Gesturing about moving the pieces was
correlated with the number of problems answered correctly ($r = .461, p <
.001$), but it was not correlated with the number of problems answered
incorrectly ($r = .202, ns$). Thus, gesturing about moving the pieces together
was uniquely related to correct performance, whereas talking about moving the
pieces was not.''
\end{quote}  

To summarize our related findings, then: in our study, gestures about movement
were the second most observed expression, after those related to perceptual
features (113 to 180); speech about movement was also second to perceptual
features in frequency (186 to 107), and neither gesture nor speech was
significantly tied to performance in our modeling tasks ($r = .29, p < .29$ for
gesture, $r= .27, p < .32$). Instead, we found the highest correlation (and
highest frequency) in speech and gestures relating to perceptual features ($r =
.61, p < .025$ for gesture, $r = .58, p < .025$ for speech). Ehrlich instead
found only a negative correlation between modeling performance and perceptual
feature coding, both from speech and from gesture.

We are then left to wonder about the rather drastic differences in our findings.
What might account for both the frequency and correlation differences in
movement versus perceptual feature strategies? Although of course we cannot know
for certain, we hinted at some of the possibilities above: the ``embodied''
nature of our tasks, having the subjects hold onto an object representative of
the solution they were striving for, the participants being' ``primed'' for
certain kinds of responses in the earlier studies, and the differences in age
all may account for some of the differences. The kind of mental processes
involved in a mental transformation task are not all that different
(necessarily) from those involved in modeling an object with the PopCAD (for
example) - a robust mental image of the shape in question is likely a boon in
either case. However, a model can be built step-by-step, and the results
observed and reflected upon. When picking a correct shape in a mental
transformation task, one may mentally operate upon features of the shape in a
step-by-step manner, but there is no opportunity to reflect upon various
strategies, a holistic decision has to be made. In a step-by-step modeling
process, it seems common (from the data and from our own experiences and
intuitions) for a modeling ``step'' to focus on a perceptual feature of the
shape being modeled (e.g. the next segment in a path or the top point in a
pyramid-shaped hull), and to do so in a very conscious way. Additionally,
subjects in our study were allowed to continue holding the model while they gave
their strategy, providing a ready ``facsimile'' on which to project their
modeling intentions. These factors may have ``paved the way'' for a high number
of speech and gesture about perceptual features of the models, as the step-wise
nature of the task and the physical surroundings lead themselves toward thinking
in terms of the characteristics of the shapes. Although we observed a high
number of expressions coded as movement, there is nothing inherently
``movement-oriented'' in 3D modeling (one's body moves in using our devices, of
course, but models can be created in other ways, e.g. strictly from text
coordinates). In contrast, a mental transformation task is, explicitly asking
the user to ``move'' the object, mentally, into the correct formation. Thus it
is unsurprising that the examiners in Ehrlich's study repeatedly used the word
``move'' and appealed to references about movement (as noted above). It is
equally unsurprising then, that by using this sort of language and then looking
at ``movement'' as a gestural and spoken strategy, many instances were found, as
the task and the instructions surrounding the task are both ``movement
oriented'' in a way that the modeling tasks in our study were not. 

One of the other major findings in \cite{ehrlich2006importance} and
\cite{levine1999early} is that significant performance differences exist between
genders on these tasks, and are evident at younger ages than previously thought.
Existing research at the time claimed that gendered differences in spatial
reasoning developed around puberty, but that several studies had challenges this
assumption. In either case, gender differences should have shown up in our study
based on the age range of our subjects (11-18). Boys did outperform girls in
session one of the modeling task,  though it was not by a terribly significant
portion (4\%), they did model faster than the girls in each round of the
modeling exercise (by about 7 minutes total in the first session, 5 minutes
overall in the second session), and they produced more speech instances than the
girls did overall (275 to 260), although since there were more boys in the
study, this advantage is negligible at best. Interestingly, our findings had
girls performing better in many areas; they outperformed boys in the second
modeling session by 9\%, and across both sessions by almost 3\%. Despite a
disadvantage in numbers, girls produced more gestures (225 to 188) including
those most closely linked to modeling success, perceptual features (96 to 84).
Girls also produced more speech elements about perceptual features (97 to 89).
The results from our mental transformation task have boys and girls performing
about equally, with girls edging out the boys by one tenth of a percent (85.3\%
to 85.2\%, respectively).

Did we have an exceptionally bright group of girls? Possibly, though no
independent tests were done for intelligence or other factors that would have
indicated an advantage - remember, the girls who enrolled in the study averaged
almost a full year younger than the boys (13 years, 7 months for girls and 14
years 6 months for boys), so age and experience advantages are unlikely (none of
the girls reported any previous 3D modeling experience). Although the nature of
the modeling exercises in our study were more piecemeal, possibly allowing girls
more of a chance to reflect and correct their mistakes than in the mental
transformation tasks\footnote{There is some evidence, relayed in
\cite{ehrlich2006importance}, that girls tend to utilize a step-by-step strategy
in mental rotation problems, whereas boys tend to deal with the whole shape at
once.}, we saw no significant difference in the MTT tasks we administered (in
fact girls did slightly better).

One possibility is that modeling with the sorts of devices we created are
somehow more beneficial to girls than to boys; that the spatial reasoning
advantages that boys have are either negated, or that the types of modeling
exercises we did significantly altered boys' normal spatial reasoning
strategies, which has been known to have a detrimental effect on
performance\cite{beilock2002paying}\cite{lutz2001procedural}. Plenty of other
possibilities exist (e.g., the girls simply tried harder) and there is no clear
way of determining the source(s) for our results, so we hesitate to make any
claims. However, we find it encouraging that girls were able to perform (even
out perform) when compared to the boys in our study.

\subsection{Commonalities}

Despite the differences mentioned in the previous section, some of our
observations did agree (or at least failed to disagree) with the previous
studies. In Ehrlich's study as well as ours, the study population improved
overall. In each case, girls improved by a markedly greater percentage, whereas
boys improved less so, and in some cases performance actually decreased (in our
second session overall and in the post-test for Ehrlich's ``Imagine Movement''
condition). Although flip-flopped in order, movement and perceptual feature
strategies were the most common, with perceptual whole instances far behind.
Generally speaking, gesture expressions deemed most ``task-appropriate'' (per
our discussion in the previous section) served as the highest observed
correlation to modeling success; perceptual feature gesturing in our study,
movement gesturing in Ehrlich's study. This is, we believe, the main ``gist'' of
both these experiments as far as gesture analysis goes - that gesture of a
strategy appropriate to the task at hand is correlated with success on that
task, more so than speech alone. This holds with the core of Goldin-Meadow's
findings, that gesture is a window into the cognitive process and that by
analyzing gesture we can gain insight into the mind of the learner.


% Girls improved, boys did not
% 
% Mis matchers?


\section{Age}

One of the more profound and noticeable results from the PopCAD and SnapCAD
study was the difference in modeling success between the devices in the first
round, and how much that difference was erased on the second round. In the first
session, users modeling with the PopCAD correctly modeled 75\% of the given
shapes, the highest percentage of any device in any round. Conversely, those
starting on the SnapCAD modeled only 34\% of their shapes properly. Given just
this data, we might be tempted to conclude that the PopCAD is a much easier
introductory device - or that the SnapCAD is insufferably difficult. However,
when we factor in the second round data, in which each subject modeled on the
device they did not use the first time around, a different picture emerges:
PopCAD modelers in the second round modeled 65\% of the shapes correctly while
SnapCAD modelers achieved a 56\% success rate. If we track each group (let us
call the first round PopCAD modelers group A, and the first round SnapCAD
modelers group B), we would be sorely tempted to declare that the groups
themselves are unevenly talented: Group A scored 75\% on PopCAD and 56\% on
SnapCAD, while group B scored 65\% on PopCAD and 34\% on SnapCAD. 

Since we did not perform a intelligence test or any sort of generalizable
aptitude test, we are left to guess using other means. Given the massive
development (both cognitively and physically) that occurs between the age
extremes in our subject population (11 to 18), it would be tempting (and even
logical) to assume that the older subjects would perform much better on the
modeling tasks than their younger counterparts. As it turns out, the average age
of group A was higher than that of group B - but only by 3 months (group A
average age was 14, group B was 13.75). We found a very modest correlation ($r =
.39, p < .15$) between age and the number of correctly modeled shapes,
suggesting that while not to be overlooked, it may play less of a role then we
would have suspected. It is also possible that the statistics are slightly
misleading here - the subject population was weighted toward the younger end of
the spectrum: the average age was 13.8, while median age was 13.5, and the mode
was 12 years old. Meaning the few older participants would have had to perform
impossibly brilliantly (i.e., higher than the highest possible score) for a
strong age to performance correlation to show up. In keeping with these
findings, we also found no real correlation between age and overall performance
on the mental transformation tasks ($r = .23, p < .45$). This data of course
does not discount that age plays a factor, nor that group A may have been more
talented than group B in the PopCAD/SnapCAD; simply that within rough
parameters, age matters, just not as much as one might think. Take for example
our oldest participant, an 18 year-old boy. He correctly performed 11 of the 24
modeling tasks, while the four 12 year-old participants scored 14, 11, 12, and
11. Our youngest participant, and 11 year old, reproduced seven shapes
correctly, while a 13 year old did five correctly, and a 14 year old got six
right.


\section{Shape Complexity}

In order to attempt to judge each shape's complexity, we sought out a
previously-defined set of criteria by which to judge ``complexity''. As it turns
out, there is a long and thorough discussion of complexity in relation to
\emph{two-dimensional} shapes, starting seemingly with Fred Attneave and Malcolm
Arnoult\cite{attneave1956quantitative}\cite{attneave1957physical} in the mid
1950's, who define methods of generating random two-dimensional shapes and
examine their physical characteristics in relation to their judged complexity.
As it turns out (in \cite{attneave1957physical} as well as others' follow-up
work) the ``Number of Turns'' in the shape was responsible for significant
amount (nearly 80\% in Attneave's study) of the preceived complexity of a shape.
``Number of Turns'' is defined as, ``the number of maxima (regardless of sign)
in one cycle of the function relating curvature to distance along the contour.
This function is a series of spikes for any angular shape, and a step-function
for any curved shape\ldots'' (see pp. 226 of the aforementioned article).
Symmetry, angular variability, and squared perimeter over area also had some
affect.

However, as it seems unclear to us how one might adapt a ``Number of Turns''
rating to a true \emph{three dimensional} model. Although many studies claim to
have studied complexity in relation to mental transformation tasks, starting
with Shepard and Metzler\cite{shepard1971mental}, who instead used perspective
line drawings, not actual physical models. This had an advantage for the types
of mental rotation tasks they were performing (recognition of matching pairs),
and similarly set off a wave of studies using the same (or similar) ``faux 3D''
stimuli\cite{metzler1974transformational}\cite{shepard1988mental}\cite{vandenberg1978mental}.

All of which leads us to determine (as best we way) the complexity of the shapes
we presented in the study, as a way of teasing out any correlation between
complexity and performance. In lieu of attempting an exact number of turns
estimate, we included three criteria: (a) the minimum number of lights necessary
to guarantee the correct shape,\footnote{In minimal spanning tree models where a
placement of lights results in several possible correct formations, only one of
which is the desired shape, we add points necessary to ``force'' the correct
representation.} (b) the number of faces (for convex hull models only), the
number of line segments (for path models only), or the number of distinct
branches (for tree models only), and (c) a symmetry score based on number of
lines of symmetry, from 3 (indicating asymmetry) to 0 (indicating 3 or more
lines of symmetry). The scaling for symmetry comes from the belief that
indicators (a) and (b) above are more closely aligned with Attneave's ``number
of turns'' metric (being highly correlated to perceived difficulty), while
symmetry was much less correlated to complexity (although symmetry did still
play a part), so we made the scale as low as possible so that it would weigh
less on the overall complexity score of a model. So, for example, a regular
octahedron would have 6 points, 8 faces, and a 0 symmetry score for an overall
difficulty score of 14. The complexity score of each shape is shown in
\ref{modelComplexity} next to the number of times it was modeled correctly. The
shapes in each session were of course different, but are labeled the same in
this table, indicating the order in which they were presented.



\begin{table}[!ht] 
\small
    \caption[Complexity of Models and Modeling Performance]{Complexity of Models
    and Modeling Performance (CH = Convex Hull, P = Path, T = Minimal Spanning
    Tree)}
    \begin{center}
    \begin{tabular}{| c | c | c | c | c | c | c | c | c | c | c | c | c | }
    \hline $ $ & CH1 & CH2 & CH3 & CH4 & P1 & P2 & P3 & P4 & T1 & T2 & T3 & T4 \\
   	\hline
   	$Session$ $1$ & & & & & & & & & & & &  \\ \hline
   	$Complexity$ $Score$ & 14 & 19 & 19 & 19 & 7 & 18 & 20 & 24 & 9 & 13 & 17 &
   	17 \\ \hline 
   	$Performance$ of 19 & 8 & 13 & 8 & 11 & 17 & 14 & 8 & 9 & 12 & 8 & 8 & 11 \\
   	\hline 
   	$Session$ $2$ & & & & & & & & & & & & \\ \hline
   	$Complexity$ $Score$ & 12 & 20 & 15 & 13 & 14 & 14 & 18 & 17 & 21 & 17 & 16
   	& 25 \\ \hline 
   	$Performance$ of 16 & 7 & 9 & 11 & 11 & 11 & 13 & 8 & 12 & 7 & 11 & 7 & 9 \\
   	\hline
   	
	\end{tabular}
   \\ \rule{0mm}{5mm}
\end{center}
\label{modelComplexity}
\end{table}


One might expect to see a strong negative correlation between a given model's
complexity score and the number of subjects who were able to model it correctly,
however the observed correlation was only moderate: $r = -0.41, p < .05$ over
both sessions.


\section{Error Analysis}

For each modeling task, one of seven error codes was recorded, based on the
outcome of the task. A more complete detail of the error codes can be found in
table \ref{modelingError}, this section focuses instead on what significance
(if any) these error codes have on our observations\footnote{Again, this data
is from the PopCAD/SnapCAD study only.}. To briefly recount the codes and their
associations, then: C = correct, EP = error in proportion (general shape is
correct, but model is too tall, too wide, etc.), E1 = error in recognition
(subject had the correct shape but did not recognize it), E2 = error in belief
(thought the shape was correct when it was not), E3 = error in implementation
(knew shape was incorrect, but knew why), E4 = error in strategy (subject knew
shape was incorrect, but could not explain why), I = incomplete (includes
giving up, asking to move on).

\begin{table}[!ht] 
\small
    \caption[Modeling Error Code Breakdown]{Error Code Breakdown. C = Correct,
    EP = Error in Proportion, E1 = Error in Recognition, E2 = Error in Belief,
    E3 = Error in Implementation, E4 = Error in Strategy, I = Incomplete.}
    \begin{center}
    \begin{tabular}{| c | c | c | c | c | c | }
    \hline $ $ & $Total$ & $PopCAD$ & $SnapCad$ & $Girls$ & $Boys$ \\ \hline
	$C$ & 221 &	98 & 123 & 100 & 121 \\ \hline
	$EP$ & 42 &	17 & 25 & 20 & 22 \\ \hline
	$E1$ & 1 &	0 &	1 &	1 &	0 \\ \hline
	$E2$ & 41 &	23 & 18 & 13 & 28 \\ \hline
	$E3$ & 2&	0&	2&	0&	2 \\ \hline
	$E4$ & 15&	11&	4&	6&	9 \\ \hline
	$I$ & 62&	40&	22&	28&	34 \\ \hline
	\end{tabular}
   \\ \rule{0mm}{5mm}
\end{center}
\label{modelErrors}
\end{table}



\section{Cross-Study Comparisons}
When we compare across studies, we see the first study resulted in 80\% (24 of
30) correct models, the second UCube study resulted in 82\% (41 of 50), while
the third study resulted in only 243/420, or about 58\%.

Complexity score


\section{Demographics}

An important factor to take into account here is the subject demographics. Only
two participants in the PopCAD/SnapCAD study had indicated previous experience
with 3D modeling in any capacity, and the study site was a drop-in program
serving primarily disadvantaged youth in a low socioeconomic neighborhood. In
contrast, the two earlier studies with the UCube were performed at a fairly
affluent, predominantly Caucasian middle school, with their multimedia class,
most of whom had been exposed to 3D modeling software (like Goggle
SketchUp\cite{SketchUp}) as part of the multimedia curriculum. So while we saw
performance in the 80\% range for the middle school and less than 60\% for our
drop-in program, not all of the difference is likely from modeling ability or
shape complexity or the ease of use of the UCube.
